{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas\n",
    "from sklearn import svm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from pylab import *\n",
    "import struct\n",
    "import keras as ks\n",
    "import logging\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from scipy import misc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取和预处理\n",
    "label 3个 => 4个\n",
    "- normal: 2462\n",
    "- cpu   : 373\n",
    "- mem   : 266\n",
    "- io    : 592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2462\n",
      "373\n",
      "266\n",
      "592\n",
      "(3693,)\n"
     ]
    }
   ],
   "source": [
    "# 3888 * 51 with 4 labels\n",
    "data_raw = pandas.read_csv(\"data/host10280-labeled-2.csv\")\n",
    "data = np.array(data_raw)\n",
    "data_raw = pandas.read_csv(\"data/host10274-labeled-2.csv\")\n",
    "data = np.concatenate((data,np.array(data_raw)),axis=0)\n",
    "data_raw = pandas.read_csv(\"data/host10283-labeled-2.csv\")\n",
    "data = np.concatenate((data,np.array(data_raw)),axis=0)\n",
    "\n",
    "# 50 = 2 ids + 45 features + 3 labels\n",
    "# ids: host + clock\n",
    "# labels: normal, cpu, mem, io\n",
    "data_features = data[:,2:47]\n",
    "data_labels = data[:,46:50]\n",
    "\n",
    "len(data_labels)\n",
    "for i in range(len(data_labels)):\n",
    "    item = data_labels[i]\n",
    "    if item[1] == 0 and item[2] == 0 and item[3] == 0:\n",
    "        data_labels[i][0] = 1\n",
    "    else:\n",
    "        data_labels[i][0] = 0\n",
    "\n",
    "data_class = [] # 0 ~ 3\n",
    "for label in data_labels:\n",
    "    data_class.append(np.dot([0,1,2,3],label))\n",
    "data_class = np.array(data_class,dtype=\"int\")\n",
    "\n",
    "print(sum(data_class == 0))\n",
    "print(sum(data_class == 1))\n",
    "print(sum(data_class == 2))\n",
    "print(sum(data_class == 3))\n",
    "print(data_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1231 abnormal\n",
    "num_sample = sum(data_class != 0)\n",
    "# select 1231 normal samples randomly\n",
    "data_tmp = data_features[data_class == 0]\n",
    "index_tmp = np.arange(0,data_tmp.shape[0],data_tmp.shape[0]/num_sample)[0:num_sample]\n",
    "data_normal_tmp = data_tmp[index_tmp]\n",
    "\n",
    "data_features = np.concatenate((data_normal_tmp,                 data_features[data_class != 0]),axis=0)\n",
    "data_labels   = np.concatenate((np.array([[1,0,0,0]]*num_sample),data_labels[data_class != 0]),  axis=0)\n",
    "data_class    = np.concatenate((np.array([0]*num_sample),        data_class[data_class != 0]),   axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分训练集和验证集\n",
    "- 训练集: 80%\n",
    "- 验证集: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1969, 45)\n",
      "(1969, 4)\n",
      "(1969,)\n",
      "(493, 45)\n",
      "(493, 4)\n",
      "(493,)\n"
     ]
    }
   ],
   "source": [
    "# feature scaling\n",
    "for i in range(data_features.shape[1]):\n",
    "    d_min = data_features[:,i].min()\n",
    "    d_max = data_features[:,i].max()\n",
    "    if d_min == d_max:\n",
    "        data_features[:,i] = 1\n",
    "        continue\n",
    "    data_features[:,i] -= d_min\n",
    "    data_features[:,i] /= (d_max - d_min)\n",
    "\n",
    "num_tosample = len(data_class)\n",
    "index_test = np.arange(0,num_tosample,5)\n",
    "index_train = np.array(list(set(np.arange(0,num_tosample,1)) - set(index_test)))\n",
    "\n",
    "data_train = data_features[index_train]\n",
    "labels_train = data_labels[index_train]\n",
    "class_train = data_class[index_train]\n",
    "data_test = data_features[index_test]\n",
    "labels_test = data_labels[index_test]\n",
    "class_test = data_class[index_test]\n",
    "print(data_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(class_train.shape)\n",
    "print(data_test.shape)\n",
    "print(labels_test.shape)\n",
    "print(class_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save loss and acc\n",
    "class LossHistory(ks.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        iindex = np.arange(0,len(self.losses[loss_type]),len(self.losses[loss_type])/200)\n",
    "        iters = np.array(iters)\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters[iindex], np.array(self.accuracy[loss_type])[iindex], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters[iindex], np.array(self.losses[loss_type])[iindex], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters[iindex], np.array(self.val_acc[loss_type])[iindex], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters[iindex], np.array(self.val_loss[loss_type])[iindex], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig('fig.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "45 => 128 => 64 => 32 => 16 => 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1969 samples, validate on 493 samples\n",
      "Epoch 1/400\n",
      "1969/1969 [==============================] - 1s - loss: 1.3323 - acc: 0.4718 - val_loss: 1.2468 - val_acc: 0.6085\n",
      "Epoch 2/400\n",
      "1969/1969 [==============================] - 0s - loss: 1.1549 - acc: 0.6247 - val_loss: 1.0164 - val_acc: 0.6126\n",
      "Epoch 3/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.8809 - acc: 0.7080 - val_loss: 0.7829 - val_acc: 0.6024\n",
      "Epoch 4/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.7075 - acc: 0.6912 - val_loss: 0.5957 - val_acc: 0.7485\n",
      "Epoch 5/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.5172 - acc: 0.7902 - val_loss: 0.4565 - val_acc: 0.8479\n",
      "Epoch 6/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.4927 - acc: 0.7913 - val_loss: 0.3974 - val_acc: 0.8580\n",
      "Epoch 7/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.3788 - acc: 0.8558 - val_loss: 0.3647 - val_acc: 0.8580\n",
      "Epoch 8/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.3502 - acc: 0.8527 - val_loss: 0.3263 - val_acc: 0.8580\n",
      "Epoch 9/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.3306 - acc: 0.8532 - val_loss: 0.3133 - val_acc: 0.8621\n",
      "Epoch 10/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.3122 - acc: 0.8558 - val_loss: 0.3097 - val_acc: 0.8621\n",
      "Epoch 11/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.3068 - acc: 0.8568 - val_loss: 0.3086 - val_acc: 0.8600\n",
      "Epoch 12/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.3045 - acc: 0.8614 - val_loss: 0.2941 - val_acc: 0.8621\n",
      "Epoch 13/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2978 - acc: 0.8583 - val_loss: 0.2913 - val_acc: 0.8621\n",
      "Epoch 14/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2972 - acc: 0.8547 - val_loss: 0.2985 - val_acc: 0.8661\n",
      "Epoch 15/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2898 - acc: 0.8568 - val_loss: 0.3157 - val_acc: 0.8621\n",
      "Epoch 16/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2904 - acc: 0.8588 - val_loss: 0.2845 - val_acc: 0.8621\n",
      "Epoch 17/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2917 - acc: 0.8558 - val_loss: 0.3002 - val_acc: 0.8053\n",
      "Epoch 18/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2857 - acc: 0.8578 - val_loss: 0.2877 - val_acc: 0.8540\n",
      "Epoch 19/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2862 - acc: 0.8619 - val_loss: 0.3024 - val_acc: 0.8235\n",
      "Epoch 20/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2877 - acc: 0.8614 - val_loss: 0.2828 - val_acc: 0.8621\n",
      "Epoch 21/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2856 - acc: 0.8608 - val_loss: 0.2843 - val_acc: 0.8357\n",
      "Epoch 22/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2841 - acc: 0.8619 - val_loss: 0.2771 - val_acc: 0.8621\n",
      "Epoch 23/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2828 - acc: 0.8619 - val_loss: 0.2864 - val_acc: 0.8621\n",
      "Epoch 24/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2846 - acc: 0.8517 - val_loss: 0.2777 - val_acc: 0.8641\n",
      "Epoch 25/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2843 - acc: 0.8558 - val_loss: 0.2802 - val_acc: 0.8621\n",
      "Epoch 26/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2808 - acc: 0.8558 - val_loss: 0.2776 - val_acc: 0.8621\n",
      "Epoch 27/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2770 - acc: 0.8629 - val_loss: 0.2901 - val_acc: 0.8600\n",
      "Epoch 28/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2775 - acc: 0.8619 - val_loss: 0.2747 - val_acc: 0.8621\n",
      "Epoch 29/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2773 - acc: 0.8614 - val_loss: 0.2783 - val_acc: 0.8621\n",
      "Epoch 30/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2776 - acc: 0.8578 - val_loss: 0.2852 - val_acc: 0.8621\n",
      "Epoch 31/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2836 - acc: 0.8578 - val_loss: 0.2734 - val_acc: 0.8621\n",
      "Epoch 32/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2748 - acc: 0.8614 - val_loss: 0.2740 - val_acc: 0.8621\n",
      "Epoch 33/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2797 - acc: 0.8583 - val_loss: 0.2776 - val_acc: 0.8621\n",
      "Epoch 34/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2791 - acc: 0.8598 - val_loss: 0.2823 - val_acc: 0.8621\n",
      "Epoch 35/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2760 - acc: 0.8603 - val_loss: 0.2802 - val_acc: 0.8783\n",
      "Epoch 36/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2769 - acc: 0.8578 - val_loss: 0.2787 - val_acc: 0.8722\n",
      "Epoch 37/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2788 - acc: 0.8608 - val_loss: 0.2724 - val_acc: 0.8641\n",
      "Epoch 38/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2733 - acc: 0.8634 - val_loss: 0.2762 - val_acc: 0.8621\n",
      "Epoch 39/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2727 - acc: 0.8588 - val_loss: 0.2725 - val_acc: 0.8621\n",
      "Epoch 40/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2715 - acc: 0.8634 - val_loss: 0.2777 - val_acc: 0.8621\n",
      "Epoch 41/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2738 - acc: 0.8624 - val_loss: 0.2825 - val_acc: 0.8235\n",
      "Epoch 42/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2738 - acc: 0.8598 - val_loss: 0.2926 - val_acc: 0.8195\n",
      "Epoch 43/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2731 - acc: 0.8603 - val_loss: 0.2750 - val_acc: 0.8621\n",
      "Epoch 44/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2733 - acc: 0.8619 - val_loss: 0.2737 - val_acc: 0.8600\n",
      "Epoch 45/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2761 - acc: 0.8558 - val_loss: 0.2742 - val_acc: 0.8600\n",
      "Epoch 46/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2703 - acc: 0.8619 - val_loss: 0.2712 - val_acc: 0.8621\n",
      "Epoch 47/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2689 - acc: 0.8629 - val_loss: 0.3026 - val_acc: 0.8600\n",
      "Epoch 48/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2701 - acc: 0.8619 - val_loss: 0.2756 - val_acc: 0.8621\n",
      "Epoch 49/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2690 - acc: 0.8644 - val_loss: 0.2798 - val_acc: 0.8621\n",
      "Epoch 50/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2720 - acc: 0.8629 - val_loss: 0.2675 - val_acc: 0.8600\n",
      "Epoch 51/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2679 - acc: 0.8578 - val_loss: 0.2700 - val_acc: 0.8621\n",
      "Epoch 52/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2686 - acc: 0.8624 - val_loss: 0.2836 - val_acc: 0.8621\n",
      "Epoch 53/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2696 - acc: 0.8669 - val_loss: 0.2683 - val_acc: 0.8621\n",
      "Epoch 54/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2669 - acc: 0.8700 - val_loss: 0.2662 - val_acc: 0.8621\n",
      "Epoch 55/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2678 - acc: 0.8563 - val_loss: 0.2704 - val_acc: 0.8702\n",
      "Epoch 56/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2644 - acc: 0.8603 - val_loss: 0.2779 - val_acc: 0.8600\n",
      "Epoch 57/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2690 - acc: 0.8608 - val_loss: 0.2631 - val_acc: 0.8621\n",
      "Epoch 58/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2670 - acc: 0.8664 - val_loss: 0.2654 - val_acc: 0.8600\n",
      "Epoch 59/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2695 - acc: 0.8629 - val_loss: 0.2658 - val_acc: 0.8621\n",
      "Epoch 60/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2673 - acc: 0.875 - 0s - loss: 0.2669 - acc: 0.8629 - val_loss: 0.2612 - val_acc: 0.8621\n",
      "Epoch 61/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2662 - acc: 0.8705 - val_loss: 0.3130 - val_acc: 0.8053\n",
      "Epoch 62/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2749 - acc: 0.8649 - val_loss: 0.2825 - val_acc: 0.8134\n",
      "Epoch 63/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2655 - acc: 0.8685 - val_loss: 0.2655 - val_acc: 0.8600\n",
      "Epoch 64/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2666 - acc: 0.8537 - val_loss: 0.2613 - val_acc: 0.8600\n",
      "Epoch 65/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2632 - acc: 0.8690 - val_loss: 0.2717 - val_acc: 0.9026\n",
      "Epoch 66/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2626 - acc: 0.8685 - val_loss: 0.2617 - val_acc: 0.8600\n",
      "Epoch 67/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2625 - acc: 0.8725 - val_loss: 0.2672 - val_acc: 0.8600\n",
      "Epoch 68/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2616 - acc: 0.8700 - val_loss: 0.2648 - val_acc: 0.8600\n",
      "Epoch 69/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2614 - acc: 0.8614 - val_loss: 0.2639 - val_acc: 0.8925\n",
      "Epoch 70/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2638 - acc: 0.8685 - val_loss: 0.2614 - val_acc: 0.8803\n",
      "Epoch 71/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2629 - acc: 0.8659 - val_loss: 0.2851 - val_acc: 0.8600\n",
      "Epoch 72/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2603 - acc: 0.8659 - val_loss: 0.3159 - val_acc: 0.8053\n",
      "Epoch 73/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2627 - acc: 0.8644 - val_loss: 0.2647 - val_acc: 0.8580\n",
      "Epoch 74/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2629 - acc: 0.8664 - val_loss: 0.2824 - val_acc: 0.8580\n",
      "Epoch 75/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2637 - acc: 0.8619 - val_loss: 0.2562 - val_acc: 0.8641\n",
      "Epoch 76/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2588 - acc: 0.8593 - val_loss: 0.2621 - val_acc: 0.8600\n",
      "Epoch 77/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2569 - acc: 0.8771 - val_loss: 0.2789 - val_acc: 0.8580\n",
      "Epoch 78/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2568 - acc: 0.8695 - val_loss: 0.2532 - val_acc: 0.8641\n",
      "Epoch 79/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2595 - acc: 0.8746 - val_loss: 0.2640 - val_acc: 0.8600\n",
      "Epoch 80/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2603 - acc: 0.8634 - val_loss: 0.2736 - val_acc: 0.8438\n",
      "Epoch 81/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2677 - acc: 0.8674 - val_loss: 0.2942 - val_acc: 0.8600\n",
      "Epoch 82/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2612 - acc: 0.8680 - val_loss: 0.2559 - val_acc: 0.8763\n",
      "Epoch 83/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2571 - acc: 0.8710 - val_loss: 0.2718 - val_acc: 0.8499\n",
      "Epoch 84/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2543 - acc: 0.8725 - val_loss: 0.2592 - val_acc: 0.8803\n",
      "Epoch 85/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2552 - acc: 0.8685 - val_loss: 0.3019 - val_acc: 0.8053\n",
      "Epoch 86/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2566 - acc: 0.8751 - val_loss: 0.2817 - val_acc: 0.8600\n",
      "Epoch 87/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2555 - acc: 0.8685 - val_loss: 0.2568 - val_acc: 0.8742\n",
      "Epoch 88/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2516 - acc: 0.8710 - val_loss: 0.2861 - val_acc: 0.8600\n",
      "Epoch 89/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2517 - acc: 0.8807 - val_loss: 0.2557 - val_acc: 0.8641\n",
      "Epoch 90/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2522 - acc: 0.8669 - val_loss: 0.2535 - val_acc: 0.8621\n",
      "Epoch 91/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2600 - acc: 0.8593 - val_loss: 0.2531 - val_acc: 0.8641\n",
      "Epoch 92/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2520 - acc: 0.8685 - val_loss: 0.2664 - val_acc: 0.8337\n",
      "Epoch 93/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2509 - acc: 0.8659 - val_loss: 0.2797 - val_acc: 0.8600\n",
      "Epoch 94/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2521 - acc: 0.8827 - val_loss: 0.2746 - val_acc: 0.8418\n",
      "Epoch 95/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2589 - acc: 0.8746 - val_loss: 0.2505 - val_acc: 0.9026\n",
      "Epoch 96/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2574 - acc: 0.8746 - val_loss: 0.2850 - val_acc: 0.8580\n",
      "Epoch 97/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2490 - acc: 0.8771 - val_loss: 0.2585 - val_acc: 0.8418\n",
      "Epoch 98/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2502 - acc: 0.8771 - val_loss: 0.2653 - val_acc: 0.8540\n",
      "Epoch 99/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2544 - acc: 0.8776 - val_loss: 0.2503 - val_acc: 0.9087\n",
      "Epoch 100/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2428 - acc: 0.8903 - val_loss: 0.2522 - val_acc: 0.9087\n",
      "Epoch 101/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2553 - acc: 0.8674 - val_loss: 0.2608 - val_acc: 0.8641\n",
      "Epoch 102/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2449 - acc: 0.8847 - val_loss: 0.2534 - val_acc: 0.8580\n",
      "Epoch 103/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2449 - acc: 0.8832 - val_loss: 0.2652 - val_acc: 0.8580\n",
      "Epoch 104/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2470 - acc: 0.8725 - val_loss: 0.3842 - val_acc: 0.8032\n",
      "Epoch 105/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2582 - acc: 0.8751 - val_loss: 0.2770 - val_acc: 0.8235\n",
      "Epoch 106/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2516 - acc: 0.8639 - val_loss: 0.2561 - val_acc: 0.8560\n",
      "Epoch 107/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2486 - acc: 0.8685 - val_loss: 0.2761 - val_acc: 0.8235\n",
      "Epoch 108/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2454 - acc: 0.8873 - val_loss: 0.2442 - val_acc: 0.9026\n",
      "Epoch 109/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2498 - acc: 0.8837 - val_loss: 0.2433 - val_acc: 0.8722\n",
      "Epoch 110/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2355 - acc: 0.8913 - val_loss: 0.2403 - val_acc: 0.9067\n",
      "Epoch 111/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2368 - acc: 0.8862 - val_loss: 0.2745 - val_acc: 0.8235\n",
      "Epoch 112/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2383 - acc: 0.8756 - val_loss: 0.2922 - val_acc: 0.8560\n",
      "Epoch 113/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2443 - acc: 0.8847 - val_loss: 0.2588 - val_acc: 0.8661\n",
      "Epoch 114/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2438 - acc: 0.8817 - val_loss: 0.2549 - val_acc: 0.8600\n",
      "Epoch 115/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2398 - acc: 0.8893 - val_loss: 0.2409 - val_acc: 0.8641\n",
      "Epoch 116/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2347 - acc: 0.8842 - val_loss: 0.2363 - val_acc: 0.9006\n",
      "Epoch 117/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2444 - acc: 0.8669 - val_loss: 0.2462 - val_acc: 0.8945\n",
      "Epoch 118/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2360 - acc: 0.8822 - val_loss: 0.2558 - val_acc: 0.8803\n",
      "Epoch 119/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2319 - acc: 0.8979 - val_loss: 0.2749 - val_acc: 0.8580\n",
      "Epoch 120/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2426 - acc: 0.8796 - val_loss: 0.2385 - val_acc: 0.8945\n",
      "Epoch 121/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2351 - acc: 0.8893 - val_loss: 0.2347 - val_acc: 0.8864\n",
      "Epoch 122/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2071 - acc: 0.898 - 0s - loss: 0.2283 - acc: 0.8984 - val_loss: 0.2780 - val_acc: 0.8357\n",
      "Epoch 123/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2401 - acc: 0.882 - 0s - loss: 0.2307 - acc: 0.8903 - val_loss: 0.2665 - val_acc: 0.8580\n",
      "Epoch 124/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2468 - acc: 0.8715 - val_loss: 0.2774 - val_acc: 0.8580\n",
      "Epoch 125/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2353 - acc: 0.8913 - val_loss: 0.3087 - val_acc: 0.8580\n",
      "Epoch 126/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2323 - acc: 0.8888 - val_loss: 0.2548 - val_acc: 0.8621\n",
      "Epoch 127/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 0s - loss: 0.2298 - acc: 0.8908 - val_loss: 0.2472 - val_acc: 0.8641\n",
      "Epoch 128/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2366 - acc: 0.8837 - val_loss: 0.2437 - val_acc: 0.8600\n",
      "Epoch 129/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2445 - acc: 0.8690 - val_loss: 0.2487 - val_acc: 0.8844\n",
      "Epoch 130/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2342 - acc: 0.8903 - val_loss: 0.2335 - val_acc: 0.8783\n",
      "Epoch 131/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2309 - acc: 0.8939 - val_loss: 0.2306 - val_acc: 0.9108\n",
      "Epoch 132/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2374 - acc: 0.8812 - val_loss: 0.2392 - val_acc: 0.9047\n",
      "Epoch 133/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.1982 - acc: 0.921 - 0s - loss: 0.2207 - acc: 0.8999 - val_loss: 0.2689 - val_acc: 0.8418\n",
      "Epoch 134/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2417 - acc: 0.8796 - val_loss: 0.2787 - val_acc: 0.8337\n",
      "Epoch 135/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2455 - acc: 0.8812 - val_loss: 0.2380 - val_acc: 0.8661\n",
      "Epoch 136/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2316 - acc: 0.8862 - val_loss: 0.2395 - val_acc: 0.8844\n",
      "Epoch 137/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2292 - acc: 0.8873 - val_loss: 0.2920 - val_acc: 0.8195\n",
      "Epoch 138/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2267 - acc: 0.8979 - val_loss: 0.2352 - val_acc: 0.9006\n",
      "Epoch 139/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2187 - acc: 0.9020 - val_loss: 0.2237 - val_acc: 0.9148\n",
      "Epoch 140/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2346 - acc: 0.8847 - val_loss: 0.2721 - val_acc: 0.8580\n",
      "Epoch 141/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2243 - acc: 0.8979 - val_loss: 0.2282 - val_acc: 0.8844\n",
      "Epoch 142/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2181 - acc: 0.8979 - val_loss: 0.2245 - val_acc: 0.9047\n",
      "Epoch 143/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2523 - acc: 0.890 - 0s - loss: 0.2356 - acc: 0.8822 - val_loss: 0.2294 - val_acc: 0.9087\n",
      "Epoch 144/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2194 - acc: 0.8994 - val_loss: 0.2322 - val_acc: 0.9067\n",
      "Epoch 145/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2154 - acc: 0.9050 - val_loss: 0.2403 - val_acc: 0.8661\n",
      "Epoch 146/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2247 - acc: 0.8928 - val_loss: 0.2154 - val_acc: 0.9108\n",
      "Epoch 147/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.1969 - acc: 0.937 - 0s - loss: 0.2156 - acc: 0.8994 - val_loss: 0.2465 - val_acc: 0.8803\n",
      "Epoch 148/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2208 - acc: 0.8888 - val_loss: 0.2273 - val_acc: 0.8682\n",
      "Epoch 149/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2238 - acc: 0.8908 - val_loss: 0.2407 - val_acc: 0.9128\n",
      "Epoch 150/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2297 - acc: 0.8939 - val_loss: 0.2363 - val_acc: 0.8783\n",
      "Epoch 151/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2156 - acc: 0.8984 - val_loss: 0.2390 - val_acc: 0.8844\n",
      "Epoch 152/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2112 - acc: 0.9081 - val_loss: 0.3004 - val_acc: 0.8235\n",
      "Epoch 153/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2146 - acc: 0.9030 - val_loss: 0.2188 - val_acc: 0.9128\n",
      "Epoch 154/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2168 - acc: 0.8954 - val_loss: 0.2432 - val_acc: 0.8641\n",
      "Epoch 155/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2220 - acc: 0.8898 - val_loss: 0.2127 - val_acc: 0.9270\n",
      "Epoch 156/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2145 - acc: 0.9010 - val_loss: 0.2471 - val_acc: 0.8621\n",
      "Epoch 157/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2119 - acc: 0.9015 - val_loss: 0.2198 - val_acc: 0.9168\n",
      "Epoch 158/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2056 - acc: 0.9096 - val_loss: 0.2179 - val_acc: 0.8925\n",
      "Epoch 159/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2179 - acc: 0.8994 - val_loss: 0.2367 - val_acc: 0.8783\n",
      "Epoch 160/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2195 - acc: 0.8949 - val_loss: 0.2083 - val_acc: 0.9067\n",
      "Epoch 161/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2078 - acc: 0.8989 - val_loss: 0.2303 - val_acc: 0.8864\n",
      "Epoch 162/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2113 - acc: 0.9025 - val_loss: 0.2830 - val_acc: 0.8560\n",
      "Epoch 163/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2078 - acc: 0.9071 - val_loss: 0.2861 - val_acc: 0.8316\n",
      "Epoch 164/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2089 - acc: 0.8984 - val_loss: 0.2111 - val_acc: 0.9128\n",
      "Epoch 165/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2041 - acc: 0.9071 - val_loss: 0.2062 - val_acc: 0.9249\n",
      "Epoch 166/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2329 - acc: 0.898 - 0s - loss: 0.2119 - acc: 0.9030 - val_loss: 0.2052 - val_acc: 0.9087\n",
      "Epoch 167/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2014 - acc: 0.9040 - val_loss: 0.2102 - val_acc: 0.9108\n",
      "Epoch 168/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2039 - acc: 0.9101 - val_loss: 0.2315 - val_acc: 0.8824\n",
      "Epoch 169/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2120 - acc: 0.8999 - val_loss: 0.2444 - val_acc: 0.8986\n",
      "Epoch 170/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2089 - acc: 0.9045 - val_loss: 0.2275 - val_acc: 0.8844\n",
      "Epoch 171/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.2201 - acc: 0.882 - 0s - loss: 0.2171 - acc: 0.8949 - val_loss: 0.2710 - val_acc: 0.8682\n",
      "Epoch 172/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2195 - acc: 0.8939 - val_loss: 0.2398 - val_acc: 0.8824\n",
      "Epoch 173/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1980 - acc: 0.9157 - val_loss: 0.2069 - val_acc: 0.9006\n",
      "Epoch 174/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1920 - acc: 0.9076 - val_loss: 0.2704 - val_acc: 0.8560\n",
      "Epoch 175/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2000 - acc: 0.9081 - val_loss: 0.2142 - val_acc: 0.8925\n",
      "Epoch 176/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2070 - acc: 0.9035 - val_loss: 0.2148 - val_acc: 0.8682\n",
      "Epoch 177/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2037 - acc: 0.8984 - val_loss: 0.2018 - val_acc: 0.9128\n",
      "Epoch 178/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1977 - acc: 0.9040 - val_loss: 0.2343 - val_acc: 0.8560\n",
      "Epoch 179/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1884 - acc: 0.9126 - val_loss: 0.2068 - val_acc: 0.9108\n",
      "Epoch 180/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2035 - acc: 0.9050 - val_loss: 0.2042 - val_acc: 0.9026\n",
      "Epoch 181/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1873 - acc: 0.9172 - val_loss: 0.2000 - val_acc: 0.9209\n",
      "Epoch 182/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1982 - acc: 0.9060 - val_loss: 0.2258 - val_acc: 0.8905\n",
      "Epoch 183/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2022 - acc: 0.9081 - val_loss: 0.2337 - val_acc: 0.8519\n",
      "Epoch 184/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1915 - acc: 0.9096 - val_loss: 0.1901 - val_acc: 0.9108\n",
      "Epoch 185/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1823 - acc: 0.9213 - val_loss: 0.2382 - val_acc: 0.8519\n",
      "Epoch 186/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1971 - acc: 0.9066 - val_loss: 0.2286 - val_acc: 0.8824\n",
      "Epoch 187/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2023 - acc: 0.9076 - val_loss: 0.2190 - val_acc: 0.8905\n",
      "Epoch 188/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1917 - acc: 0.9066 - val_loss: 0.2863 - val_acc: 0.8418\n",
      "Epoch 189/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1895 - acc: 0.9121 - val_loss: 0.2696 - val_acc: 0.8560\n",
      "Epoch 190/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1879 - acc: 0.9096 - val_loss: 0.2258 - val_acc: 0.8864\n",
      "Epoch 191/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1881 - acc: 0.9116 - val_loss: 0.2487 - val_acc: 0.8499\n",
      "Epoch 192/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1871 - acc: 0.9167 - val_loss: 0.1927 - val_acc: 0.9229\n",
      "Epoch 193/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1901 - acc: 0.9121 - val_loss: 0.2302 - val_acc: 0.8824\n",
      "Epoch 194/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.2012 - acc: 0.9055 - val_loss: 0.2107 - val_acc: 0.8945\n",
      "Epoch 195/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1808 - acc: 0.9167 - val_loss: 0.1968 - val_acc: 0.9108\n",
      "Epoch 196/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1929 - acc: 0.9086 - val_loss: 0.2404 - val_acc: 0.8519\n",
      "Epoch 197/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1949 - acc: 0.9040 - val_loss: 0.1851 - val_acc: 0.9331\n",
      "Epoch 198/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1815 - acc: 0.9177 - val_loss: 0.1945 - val_acc: 0.9108\n",
      "Epoch 199/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1824 - acc: 0.9157 - val_loss: 0.2395 - val_acc: 0.9209\n",
      "Epoch 200/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1871 - acc: 0.9192 - val_loss: 0.2466 - val_acc: 0.8600\n",
      "Epoch 201/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1921 - acc: 0.9055 - val_loss: 0.2016 - val_acc: 0.8722\n",
      "Epoch 202/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1820 - acc: 0.9126 - val_loss: 0.2067 - val_acc: 0.8966\n",
      "Epoch 203/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1968 - acc: 0.9121 - val_loss: 0.1868 - val_acc: 0.9128\n",
      "Epoch 204/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1839 - acc: 0.9198 - val_loss: 0.1837 - val_acc: 0.9108\n",
      "Epoch 205/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1758 - acc: 0.9152 - val_loss: 0.1789 - val_acc: 0.9351\n",
      "Epoch 206/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1868 - acc: 0.9157 - val_loss: 0.1851 - val_acc: 0.9249\n",
      "Epoch 207/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1782 - acc: 0.9147 - val_loss: 0.2159 - val_acc: 0.8844\n",
      "Epoch 208/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1803 - acc: 0.9167 - val_loss: 0.1991 - val_acc: 0.9087\n",
      "Epoch 209/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1711 - acc: 0.9238 - val_loss: 0.2096 - val_acc: 0.9067\n",
      "Epoch 210/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1760 - acc: 0.9228 - val_loss: 0.2401 - val_acc: 0.8560\n",
      "Epoch 211/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1859 - acc: 0.9096 - val_loss: 0.1819 - val_acc: 0.9249\n",
      "Epoch 212/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1701 - acc: 0.9284 - val_loss: 0.1767 - val_acc: 0.9209\n",
      "Epoch 213/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1758 - acc: 0.9223 - val_loss: 0.2276 - val_acc: 0.8641\n",
      "Epoch 214/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1790 - acc: 0.9152 - val_loss: 0.2045 - val_acc: 0.8884\n",
      "Epoch 215/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1641 - acc: 0.9304 - val_loss: 0.2127 - val_acc: 0.8925\n",
      "Epoch 216/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1782 - acc: 0.9126 - val_loss: 0.2070 - val_acc: 0.8884\n",
      "Epoch 217/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1658 - acc: 0.9289 - val_loss: 0.1888 - val_acc: 0.9168\n",
      "Epoch 218/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1835 - acc: 0.9101 - val_loss: 0.2505 - val_acc: 0.8925\n",
      "Epoch 219/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1752 - acc: 0.9152 - val_loss: 0.2018 - val_acc: 0.8783\n",
      "Epoch 220/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1653 - acc: 0.9319 - val_loss: 0.1998 - val_acc: 0.8986\n",
      "Epoch 221/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1621 - acc: 0.9208 - val_loss: 0.2391 - val_acc: 0.8600\n",
      "Epoch 222/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1800 - acc: 0.9182 - val_loss: 0.1819 - val_acc: 0.9229\n",
      "Epoch 223/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1775 - acc: 0.9162 - val_loss: 0.2347 - val_acc: 0.8844\n",
      "Epoch 224/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1666 - acc: 0.9218 - val_loss: 0.2002 - val_acc: 0.8966\n",
      "Epoch 225/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1686 - acc: 0.9243 - val_loss: 0.1870 - val_acc: 0.9148\n",
      "Epoch 226/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1784 - acc: 0.9177 - val_loss: 0.1809 - val_acc: 0.9391\n",
      "Epoch 227/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1665 - acc: 0.9253 - val_loss: 0.1741 - val_acc: 0.9148\n",
      "Epoch 228/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1591 - acc: 0.9233 - val_loss: 0.1699 - val_acc: 0.9391\n",
      "Epoch 229/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1680 - acc: 0.9233 - val_loss: 0.1917 - val_acc: 0.9128\n",
      "Epoch 230/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1664 - acc: 0.9304 - val_loss: 0.1697 - val_acc: 0.9351\n",
      "Epoch 231/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1694 - acc: 0.9223 - val_loss: 0.1947 - val_acc: 0.9168\n",
      "Epoch 232/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1608 - acc: 0.9259 - val_loss: 0.1642 - val_acc: 0.9189\n",
      "Epoch 233/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1919 - acc: 0.9101 - val_loss: 0.2041 - val_acc: 0.9047\n",
      "Epoch 234/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1593 - acc: 0.9289 - val_loss: 0.2877 - val_acc: 0.8621\n",
      "Epoch 235/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1777 - acc: 0.9147 - val_loss: 0.2041 - val_acc: 0.9108\n",
      "Epoch 236/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1621 - acc: 0.9279 - val_loss: 0.1874 - val_acc: 0.9067\n",
      "Epoch 237/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1623 - acc: 0.9203 - val_loss: 0.1810 - val_acc: 0.9087\n",
      "Epoch 238/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1533 - acc: 0.9304 - val_loss: 0.2074 - val_acc: 0.9148\n",
      "Epoch 239/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1593 - acc: 0.9304 - val_loss: 0.1943 - val_acc: 0.9047\n",
      "Epoch 240/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1747 - acc: 0.9157 - val_loss: 0.1661 - val_acc: 0.9168\n",
      "Epoch 241/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1575 - acc: 0.9248 - val_loss: 0.1966 - val_acc: 0.9148\n",
      "Epoch 242/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1642 - acc: 0.9269 - val_loss: 0.2257 - val_acc: 0.8722\n",
      "Epoch 243/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1621 - acc: 0.9177 - val_loss: 0.2182 - val_acc: 0.9148\n",
      "Epoch 244/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1538 - acc: 0.9243 - val_loss: 0.2160 - val_acc: 0.8925\n",
      "Epoch 245/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1809 - acc: 0.9162 - val_loss: 0.2447 - val_acc: 0.8824\n",
      "Epoch 246/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1659 - acc: 0.9213 - val_loss: 0.1942 - val_acc: 0.9067\n",
      "Epoch 247/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1561 - acc: 0.9223 - val_loss: 0.2949 - val_acc: 0.8519\n",
      "Epoch 248/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1671 - acc: 0.9233 - val_loss: 0.2224 - val_acc: 0.8803\n",
      "Epoch 249/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1532 - acc: 0.9335 - val_loss: 0.2916 - val_acc: 0.8499\n",
      "Epoch 250/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1656 - acc: 0.9218 - val_loss: 0.1914 - val_acc: 0.9148\n",
      "Epoch 251/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1726 - acc: 0.9121 - val_loss: 0.2560 - val_acc: 0.8986\n",
      "Epoch 252/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1500 - acc: 0.9375 - val_loss: 0.2227 - val_acc: 0.9148\n",
      "Epoch 253/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 0s - loss: 0.1567 - acc: 0.9304 - val_loss: 0.1749 - val_acc: 0.9249\n",
      "Epoch 254/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1408 - acc: 0.9385 - val_loss: 0.2258 - val_acc: 0.8722\n",
      "Epoch 255/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1685 - acc: 0.9198 - val_loss: 0.1623 - val_acc: 0.9229\n",
      "Epoch 256/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1614 - acc: 0.9162 - val_loss: 0.1944 - val_acc: 0.9067\n",
      "Epoch 257/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1704 - acc: 0.9172 - val_loss: 0.1823 - val_acc: 0.9148\n",
      "Epoch 258/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1464 - acc: 0.9365 - val_loss: 0.2415 - val_acc: 0.9026\n",
      "Epoch 259/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1664 - acc: 0.9157 - val_loss: 0.1787 - val_acc: 0.9067\n",
      "Epoch 260/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1469 - acc: 0.9360 - val_loss: 0.1787 - val_acc: 0.9209\n",
      "Epoch 261/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1659 - acc: 0.9157 - val_loss: 0.2320 - val_acc: 0.8682\n",
      "Epoch 262/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1595 - acc: 0.9172 - val_loss: 0.1829 - val_acc: 0.9087\n",
      "Epoch 263/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1552 - acc: 0.9223 - val_loss: 0.2089 - val_acc: 0.9067\n",
      "Epoch 264/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1459 - acc: 0.9325 - val_loss: 0.1644 - val_acc: 0.9229\n",
      "Epoch 265/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1465 - acc: 0.9360 - val_loss: 0.1573 - val_acc: 0.9371\n",
      "Epoch 266/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1516 - acc: 0.9314 - val_loss: 0.2951 - val_acc: 0.8824\n",
      "Epoch 267/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1550 - acc: 0.9274 - val_loss: 0.2014 - val_acc: 0.8966\n",
      "Epoch 268/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1504 - acc: 0.9274 - val_loss: 0.1816 - val_acc: 0.9249\n",
      "Epoch 269/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1460 - acc: 0.9335 - val_loss: 0.1595 - val_acc: 0.9412\n",
      "Epoch 270/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1510 - acc: 0.9335 - val_loss: 0.1674 - val_acc: 0.9229\n",
      "Epoch 271/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1536 - acc: 0.9259 - val_loss: 0.1819 - val_acc: 0.9189\n",
      "Epoch 272/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1627 - acc: 0.9284 - val_loss: 0.2264 - val_acc: 0.8864\n",
      "Epoch 273/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1643 - acc: 0.9218 - val_loss: 0.1522 - val_acc: 0.9331\n",
      "Epoch 274/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1622 - acc: 0.9233 - val_loss: 0.1603 - val_acc: 0.9331\n",
      "Epoch 275/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1554 - acc: 0.9294 - val_loss: 0.1827 - val_acc: 0.9087\n",
      "Epoch 276/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1480 - acc: 0.9340 - val_loss: 0.1534 - val_acc: 0.9209\n",
      "Epoch 277/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1440 - acc: 0.9345 - val_loss: 0.2121 - val_acc: 0.9067\n",
      "Epoch 278/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1523 - acc: 0.9309 - val_loss: 0.1825 - val_acc: 0.9209\n",
      "Epoch 279/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1510 - acc: 0.9309 - val_loss: 0.1736 - val_acc: 0.9209\n",
      "Epoch 280/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1524 - acc: 0.9294 - val_loss: 0.1759 - val_acc: 0.9189\n",
      "Epoch 281/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1490 - acc: 0.9269 - val_loss: 0.1766 - val_acc: 0.9087\n",
      "Epoch 282/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1445 - acc: 0.9294 - val_loss: 0.1628 - val_acc: 0.9290\n",
      "Epoch 283/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1302 - acc: 0.9426 - val_loss: 0.2236 - val_acc: 0.9026\n",
      "Epoch 284/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1673 - acc: 0.9223 - val_loss: 0.1697 - val_acc: 0.9087\n",
      "Epoch 285/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1633 - acc: 0.9167 - val_loss: 0.1812 - val_acc: 0.9249\n",
      "Epoch 286/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1478 - acc: 0.9319 - val_loss: 0.1621 - val_acc: 0.9331\n",
      "Epoch 287/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1521 - acc: 0.9264 - val_loss: 0.2219 - val_acc: 0.9067\n",
      "Epoch 288/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1560 - acc: 0.9269 - val_loss: 0.1561 - val_acc: 0.9209\n",
      "Epoch 289/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1368 - acc: 0.9340 - val_loss: 0.1769 - val_acc: 0.9148\n",
      "Epoch 290/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1593 - acc: 0.9299 - val_loss: 0.1622 - val_acc: 0.9310\n",
      "Epoch 291/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1399 - acc: 0.9335 - val_loss: 0.2168 - val_acc: 0.9067\n",
      "Epoch 292/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1583 - acc: 0.9253 - val_loss: 0.2009 - val_acc: 0.9148\n",
      "Epoch 293/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1533 - acc: 0.9243 - val_loss: 0.1775 - val_acc: 0.9209\n",
      "Epoch 294/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1615 - acc: 0.9218 - val_loss: 0.1608 - val_acc: 0.9249\n",
      "Epoch 295/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1424 - acc: 0.9294 - val_loss: 0.2180 - val_acc: 0.8844\n",
      "Epoch 296/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1444 - acc: 0.9279 - val_loss: 0.1531 - val_acc: 0.9189\n",
      "Epoch 297/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1346 - acc: 0.9380 - val_loss: 0.1758 - val_acc: 0.9108\n",
      "Epoch 298/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1351 - acc: 0.9396 - val_loss: 0.1896 - val_acc: 0.9189\n",
      "Epoch 299/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1399 - acc: 0.9355 - val_loss: 0.1767 - val_acc: 0.9189\n",
      "Epoch 300/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1604 - acc: 0.9233 - val_loss: 0.1524 - val_acc: 0.9229\n",
      "Epoch 301/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1299 - acc: 0.9396 - val_loss: 0.1693 - val_acc: 0.9249\n",
      "Epoch 302/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1388 - acc: 0.9340 - val_loss: 0.1606 - val_acc: 0.9310\n",
      "Epoch 303/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1516 - acc: 0.9304 - val_loss: 0.1742 - val_acc: 0.9168\n",
      "Epoch 304/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1480 - acc: 0.9259 - val_loss: 0.1780 - val_acc: 0.9270\n",
      "Epoch 305/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1302 - acc: 0.9421 - val_loss: 0.1440 - val_acc: 0.9412\n",
      "Epoch 306/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1538 - acc: 0.9264 - val_loss: 0.2024 - val_acc: 0.9148\n",
      "Epoch 307/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1303 - acc: 0.9421 - val_loss: 0.1439 - val_acc: 0.9371\n",
      "Epoch 308/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1316 - acc: 0.9385 - val_loss: 0.2221 - val_acc: 0.8966\n",
      "Epoch 309/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1571 - acc: 0.9228 - val_loss: 0.2413 - val_acc: 0.8844\n",
      "Epoch 310/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1266 - acc: 0.9446 - val_loss: 0.1442 - val_acc: 0.9412\n",
      "Epoch 311/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1363 - acc: 0.9350 - val_loss: 0.1811 - val_acc: 0.9168\n",
      "Epoch 312/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1469 - acc: 0.9340 - val_loss: 0.1779 - val_acc: 0.9189\n",
      "Epoch 313/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1287 - acc: 0.9380 - val_loss: 0.2892 - val_acc: 0.8763\n",
      "Epoch 314/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1407 - acc: 0.9309 - val_loss: 0.1490 - val_acc: 0.9371\n",
      "Epoch 315/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1297 - acc: 0.9391 - val_loss: 0.1683 - val_acc: 0.9310\n",
      "Epoch 316/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1189 - acc: 0.9482 - val_loss: 0.1427 - val_acc: 0.9412\n",
      "Epoch 317/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1320 - acc: 0.9330 - val_loss: 0.1809 - val_acc: 0.9128\n",
      "Epoch 318/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1631 - acc: 0.9187 - val_loss: 0.1599 - val_acc: 0.9310\n",
      "Epoch 319/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1371 - acc: 0.9330 - val_loss: 0.2104 - val_acc: 0.9209\n",
      "Epoch 320/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1431 - acc: 0.9335 - val_loss: 0.2076 - val_acc: 0.9087\n",
      "Epoch 321/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1326 - acc: 0.9375 - val_loss: 0.1582 - val_acc: 0.9128\n",
      "Epoch 322/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1434 - acc: 0.9380 - val_loss: 0.1645 - val_acc: 0.9209\n",
      "Epoch 323/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1439 - acc: 0.9319 - val_loss: 0.1591 - val_acc: 0.9290\n",
      "Epoch 324/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1312 - acc: 0.9401 - val_loss: 0.1556 - val_acc: 0.9290\n",
      "Epoch 325/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1396 - acc: 0.9355 - val_loss: 0.2161 - val_acc: 0.9006\n",
      "Epoch 326/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1286 - acc: 0.9365 - val_loss: 0.2165 - val_acc: 0.9067\n",
      "Epoch 327/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1357 - acc: 0.9365 - val_loss: 0.1850 - val_acc: 0.9148\n",
      "Epoch 328/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1327 - acc: 0.9365 - val_loss: 0.2056 - val_acc: 0.9168\n",
      "Epoch 329/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1403 - acc: 0.9340 - val_loss: 0.1421 - val_acc: 0.9432\n",
      "Epoch 330/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1249 - acc: 0.9421 - val_loss: 0.1895 - val_acc: 0.9189\n",
      "Epoch 331/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1413 - acc: 0.9380 - val_loss: 0.1485 - val_acc: 0.9432\n",
      "Epoch 332/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1248 - acc: 0.9441 - val_loss: 0.2601 - val_acc: 0.8966\n",
      "Epoch 333/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1380 - acc: 0.9330 - val_loss: 0.2712 - val_acc: 0.9108\n",
      "Epoch 334/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1328 - acc: 0.9416 - val_loss: 0.1908 - val_acc: 0.9168\n",
      "Epoch 335/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1398 - acc: 0.9355 - val_loss: 0.1391 - val_acc: 0.9290\n",
      "Epoch 336/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1251 - acc: 0.9411 - val_loss: 0.2140 - val_acc: 0.9229\n",
      "Epoch 337/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1373 - acc: 0.9370 - val_loss: 0.1389 - val_acc: 0.9473\n",
      "Epoch 338/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1200 - acc: 0.9421 - val_loss: 0.2240 - val_acc: 0.9047\n",
      "Epoch 339/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1618 - acc: 0.9248 - val_loss: 0.1943 - val_acc: 0.9108\n",
      "Epoch 340/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1301 - acc: 0.9411 - val_loss: 0.1370 - val_acc: 0.9533\n",
      "Epoch 341/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1290 - acc: 0.9451 - val_loss: 0.2176 - val_acc: 0.9067\n",
      "Epoch 342/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1582 - acc: 0.9208 - val_loss: 0.1427 - val_acc: 0.9412\n",
      "Epoch 343/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1307 - acc: 0.9345 - val_loss: 0.1431 - val_acc: 0.9493\n",
      "Epoch 344/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1291 - acc: 0.9426 - val_loss: 0.1421 - val_acc: 0.9351\n",
      "Epoch 345/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1324 - acc: 0.9355 - val_loss: 0.1558 - val_acc: 0.9310\n",
      "Epoch 346/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1269 - acc: 0.9421 - val_loss: 0.1660 - val_acc: 0.9148\n",
      "Epoch 347/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1422 - acc: 0.9269 - val_loss: 0.1720 - val_acc: 0.9189\n",
      "Epoch 348/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1215 - acc: 0.9446 - val_loss: 0.1465 - val_acc: 0.9351\n",
      "Epoch 349/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1371 - acc: 0.9319 - val_loss: 0.2353 - val_acc: 0.8945\n",
      "Epoch 350/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1231 - acc: 0.9477 - val_loss: 0.1965 - val_acc: 0.9168\n",
      "Epoch 351/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1471 - acc: 0.9284 - val_loss: 0.3098 - val_acc: 0.8783\n",
      "Epoch 352/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1363 - acc: 0.9350 - val_loss: 0.1898 - val_acc: 0.9249\n",
      "Epoch 353/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1422 - acc: 0.9350 - val_loss: 0.1500 - val_acc: 0.9331\n",
      "Epoch 354/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1115 - acc: 0.9538 - val_loss: 0.2618 - val_acc: 0.8864\n",
      "Epoch 355/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1483 - acc: 0.9304 - val_loss: 0.1321 - val_acc: 0.9351\n",
      "Epoch 356/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1308 - acc: 0.9401 - val_loss: 0.1983 - val_acc: 0.9168\n",
      "Epoch 357/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1212 - acc: 0.9431 - val_loss: 0.1425 - val_acc: 0.9452\n",
      "Epoch 358/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1139 - acc: 0.9523 - val_loss: 0.1854 - val_acc: 0.9148\n",
      "Epoch 359/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1156 - acc: 0.9497 - val_loss: 0.1741 - val_acc: 0.9229\n",
      "Epoch 360/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1309 - acc: 0.9365 - val_loss: 0.1679 - val_acc: 0.9189\n",
      "Epoch 361/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1202 - acc: 0.9411 - val_loss: 0.1518 - val_acc: 0.9351\n",
      "Epoch 362/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1385 - acc: 0.9345 - val_loss: 0.1700 - val_acc: 0.9290\n",
      "Epoch 363/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1166 - acc: 0.9446 - val_loss: 0.1570 - val_acc: 0.9371\n",
      "Epoch 364/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1447 - acc: 0.9299 - val_loss: 0.1569 - val_acc: 0.9249\n",
      "Epoch 365/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1138 - acc: 0.9477 - val_loss: 0.1328 - val_acc: 0.9473\n",
      "Epoch 366/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1203 - acc: 0.9462 - val_loss: 0.1379 - val_acc: 0.9371\n",
      "Epoch 367/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1326 - acc: 0.9325 - val_loss: 0.2381 - val_acc: 0.8925\n",
      "Epoch 368/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1567 - acc: 0.9218 - val_loss: 0.2000 - val_acc: 0.9209\n",
      "Epoch 369/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1468 - acc: 0.9304 - val_loss: 0.2542 - val_acc: 0.9026\n",
      "Epoch 370/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1461 - acc: 0.9319 - val_loss: 0.1422 - val_acc: 0.9493\n",
      "Epoch 371/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1210 - acc: 0.9446 - val_loss: 0.1570 - val_acc: 0.9331\n",
      "Epoch 372/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1288 - acc: 0.9355 - val_loss: 0.2125 - val_acc: 0.9148\n",
      "Epoch 373/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1350 - acc: 0.9330 - val_loss: 0.1772 - val_acc: 0.9229\n",
      "Epoch 374/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1261 - acc: 0.9406 - val_loss: 0.1551 - val_acc: 0.9310\n",
      "Epoch 375/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1253 - acc: 0.9370 - val_loss: 0.2325 - val_acc: 0.9189\n",
      "Epoch 376/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1338 - acc: 0.9385 - val_loss: 0.1465 - val_acc: 0.9290\n",
      "Epoch 377/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1282 - acc: 0.9391 - val_loss: 0.1477 - val_acc: 0.9473\n",
      "Epoch 378/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1320 - acc: 0.9391 - val_loss: 0.2184 - val_acc: 0.9087\n",
      "Epoch 379/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1201 - acc: 0.9467 - val_loss: 0.1374 - val_acc: 0.9391\n",
      "Epoch 380/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1174 - acc: 0.9446 - val_loss: 0.1360 - val_acc: 0.9432\n",
      "Epoch 381/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969/1969 [==============================] - 0s - loss: 0.1260 - acc: 0.9401 - val_loss: 0.1988 - val_acc: 0.9148\n",
      "Epoch 382/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1298 - acc: 0.9406 - val_loss: 0.1650 - val_acc: 0.9270\n",
      "Epoch 383/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1333 - acc: 0.9375 - val_loss: 0.1597 - val_acc: 0.9249\n",
      "Epoch 384/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1281 - acc: 0.9406 - val_loss: 0.1564 - val_acc: 0.9310\n",
      "Epoch 385/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1198 - acc: 0.9462 - val_loss: 0.1329 - val_acc: 0.9513\n",
      "Epoch 386/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1497 - acc: 0.9289 - val_loss: 0.1631 - val_acc: 0.9229\n",
      "Epoch 387/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1315 - acc: 0.9411 - val_loss: 0.2254 - val_acc: 0.9006\n",
      "Epoch 388/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1326 - acc: 0.9355 - val_loss: 0.1606 - val_acc: 0.9229\n",
      "Epoch 389/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1320 - acc: 0.9314 - val_loss: 0.1486 - val_acc: 0.9371\n",
      "Epoch 390/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1150 - acc: 0.9441 - val_loss: 0.1437 - val_acc: 0.9270\n",
      "Epoch 391/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1217 - acc: 0.9391 - val_loss: 0.1301 - val_acc: 0.9371\n",
      "Epoch 392/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1157 - acc: 0.9446 - val_loss: 0.2054 - val_acc: 0.9168\n",
      "Epoch 393/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1309 - acc: 0.9380 - val_loss: 0.1802 - val_acc: 0.9229\n",
      "Epoch 394/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1286 - acc: 0.9401 - val_loss: 0.2139 - val_acc: 0.9108\n",
      "Epoch 395/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1139 - acc: 0.9467 - val_loss: 0.1429 - val_acc: 0.9432\n",
      "Epoch 396/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1409 - acc: 0.9355 - val_loss: 0.1548 - val_acc: 0.9249\n",
      "Epoch 397/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1298 - acc: 0.9375 - val_loss: 0.1524 - val_acc: 0.9371\n",
      "Epoch 398/400\n",
      "1969/1969 [==============================] - ETA: 0s - loss: 0.1216 - acc: 0.945 - 0s - loss: 0.1110 - acc: 0.9446 - val_loss: 0.1405 - val_acc: 0.9371\n",
      "Epoch 399/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1355 - acc: 0.9416 - val_loss: 0.1900 - val_acc: 0.9128\n",
      "Epoch 400/400\n",
      "1969/1969 [==============================] - 0s - loss: 0.1148 - acc: 0.9472 - val_loss: 0.1678 - val_acc: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c24ca5110>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ks.models.Sequential()\n",
    "model.add(Dense(128, input_dim=data_features.shape[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "history = LossHistory()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "model.fit(x=data_train,y=labels_train,batch_size=128,nb_epoch=5000,verbose=1,validation_data=(data_test,labels_test),callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c25ae8b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,data):\n",
    "    tmp_res = model.predict(data)\n",
    "    res = []\n",
    "    for item in tmp_res:\n",
    "        res.append(np.where(item == max(item))[0][0])\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.987829614604\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       984\n",
      "        cpu       0.96      1.00      0.98       299\n",
      "        mem       1.00      0.96      0.98       212\n",
      "         io       1.00      0.99      1.00       474\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1969\n",
      "\n",
      "test: 0.983772819473\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       247\n",
      "        cpu       0.93      1.00      0.96        74\n",
      "        mem       1.00      0.93      0.96        54\n",
      "         io       0.98      0.97      0.97       118\n",
      "\n",
      "avg / total       0.98      0.98      0.98       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = class_train\n",
    "y_pred = predict(model,data_train)\n",
    "target_names = ['normal', 'cpu', 'mem', 'io']\n",
    "print(\"train: {}\".format(clf.score(mid_features_test, class_test)))\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# print(\"test: {}\".format(clf.score(mid_features_test, class_test)))\n",
    "y_true = class_test\n",
    "y_pred = predict(model,data_test)\n",
    "print(\"test: {}\".format(sum(y_true == y_pred)/float(len(y_true))))\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络 + SVM\n",
    "45 => 128 => 64 => 32 => 16 => SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98781107161\n",
      "0.987829614604\n",
      "train: 0.98781107161\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       984\n",
      "        cpu       0.98      0.95      0.96       299\n",
      "        mem       1.00      0.98      0.99       212\n",
      "         io       0.96      0.99      0.98       474\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1969\n",
      "\n",
      "test: 0.987829614604\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       247\n",
      "        cpu       0.99      0.96      0.97        74\n",
      "        mem       1.00      0.96      0.98        54\n",
      "         io       0.96      0.99      0.97       118\n",
      "\n",
      "avg / total       0.99      0.99      0.99       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_feature = K.function([model.layers[0].input],[model.layers[4].output])\n",
    "mid_features_train = get_feature([data_train])[0]\n",
    "mid_features_test  = get_feature([data_test])[0]\n",
    "clf = svm.SVC()\n",
    "clf.fit(mid_features_train, class_train)\n",
    "\n",
    "print(\"train: {}\".format(clf.score(mid_features_train, class_train)))\n",
    "y_true = class_train\n",
    "y_pred = clf.predict(mid_features_train)\n",
    "target_names = ['normal', 'cpu', 'mem', 'io']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "print(\"test: {}\".format(clf.score(mid_features_test, class_test)))\n",
    "y_true = class_test\n",
    "y_pred = clf.predict(mid_features_test)\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他机器学习算法\n",
    "- KNN\n",
    "- 线性SVM\n",
    "- 基于核函数的SVM (RBF SVM), Radial basis function kernel\n",
    "- 决策树\n",
    "- 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "train: 0.973590655155\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       984\n",
      "        cpu       0.89      0.97      0.93       299\n",
      "        mem       0.95      0.90      0.92       212\n",
      "         io       0.98      0.96      0.97       474\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1969\n",
      "\n",
      "test: 0.95537525355\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       247\n",
      "        cpu       0.84      0.93      0.88        74\n",
      "        mem       0.94      0.83      0.88        54\n",
      "         io       0.95      0.93      0.94       118\n",
      "\n",
      "avg / total       0.96      0.96      0.96       493\n",
      "\n",
      "\n",
      "\n",
      "Linear SVM\n",
      "train: 0.857287963433\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       984\n",
      "        cpu       1.00      0.45      0.63       299\n",
      "        mem       1.00      0.44      0.61       212\n",
      "         io       0.63      1.00      0.77       474\n",
      "\n",
      "avg / total       0.91      0.86      0.85      1969\n",
      "\n",
      "test: 0.855983772819\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       247\n",
      "        cpu       1.00      0.45      0.62        74\n",
      "        mem       1.00      0.44      0.62        54\n",
      "         io       0.62      1.00      0.77       118\n",
      "\n",
      "avg / total       0.91      0.86      0.85       493\n",
      "\n",
      "\n",
      "\n",
      "RBF SVM\n",
      "train: 0.858303707466\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       984\n",
      "        cpu       0.99      0.46      0.63       299\n",
      "        mem       1.00      0.44      0.61       212\n",
      "         io       0.63      1.00      0.77       474\n",
      "\n",
      "avg / total       0.91      0.86      0.85      1969\n",
      "\n",
      "test: 0.860040567951\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       247\n",
      "        cpu       1.00      0.47      0.64        74\n",
      "        mem       1.00      0.44      0.62        54\n",
      "         io       0.63      1.00      0.77       118\n",
      "\n",
      "avg / total       0.91      0.86      0.85       493\n",
      "\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "train: 0.861350939563\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       984\n",
      "        cpu       1.00      0.80      0.89       299\n",
      "        mem       0.00      0.00      0.00       212\n",
      "         io       0.63      1.00      0.78       474\n",
      "\n",
      "avg / total       0.80      0.86      0.82      1969\n",
      "\n",
      "test: 0.860040567951\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      1.00      1.00       247\n",
      "        cpu       1.00      0.80      0.89        74\n",
      "        mem       0.00      0.00      0.00        54\n",
      "         io       0.63      1.00      0.77       118\n",
      "\n",
      "avg / total       0.80      0.86      0.82       493\n",
      "\n",
      "\n",
      "\n",
      "Random Forest\n",
      "train: 0.919756221432\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       0.92      0.98      0.95       984\n",
      "        cpu       0.97      0.83      0.89       299\n",
      "        mem       1.00      0.63      0.77       212\n",
      "         io       0.87      0.99      0.93       474\n",
      "\n",
      "avg / total       0.93      0.92      0.92      1969\n",
      "\n",
      "test: 0.914807302231\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       0.92      0.97      0.94       247\n",
      "        cpu       0.98      0.82      0.90        74\n",
      "        mem       1.00      0.63      0.77        54\n",
      "         io       0.85      0.98      0.91       118\n",
      "\n",
      "avg / total       0.92      0.91      0.91       493\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(4),\n",
    "    svm.SVC(kernel=\"linear\", C=0.025),\n",
    "    svm.SVC(),\n",
    "    DecisionTreeClassifier(max_depth=3),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "    ]\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\"]\n",
    "for name, clf in zip(names,classifiers):\n",
    "    clf.fit(data_train,class_train)\n",
    "    print(name)\n",
    "    print(\"train: {}\".format(clf.score(data_train, class_train)))\n",
    "    y_true = class_train\n",
    "    y_pred = clf.predict(data_train)\n",
    "    target_names = ['normal', 'cpu', 'mem', 'io']\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n",
    "    print(\"test: {}\".format(clf.score(data_test, class_test)))\n",
    "    y_true = class_test\n",
    "    y_pred = clf.predict(data_test)\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
